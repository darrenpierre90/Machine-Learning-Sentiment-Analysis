{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP4900A2Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darrenpierre90/Machine-Learning-Sentiment-Analysis/blob/master/COMP4900A2Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r0KBuYljUV0",
        "colab_type": "code",
        "outputId": "7efec204-c5b7-44ae-b4f5-e8c0450082ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "drive.mount('/content/drive')\n",
        "TRAIN_DATA=\"/content/drive/My Drive/Colab Notebooks/Data Sets/train.csv\"\n",
        "TEST_DATA=\"/content/drive/My Drive/Colab Notebooks/Data Sets/test.csv\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BmTUF0EDM81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PuUUVmHl14p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading up datasets into dataframes\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "trainDS=pd.read_csv(TRAIN_DATA)\n",
        "testDS=pd.read_csv(TEST_DATA)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt7nMyZitdtF",
        "colab_type": "code",
        "outputId": "69992af2-8f9b-4053-e8c1-ae84e8fe2641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# Feature engineering test\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "X_train, X_test, y_train, y_test = train_test_split(trainDS.review, trainDS.sentiment, train_size=0.80, test_size=0.20)\n",
        "set_of_vectorizers=[CountVectorizer(max_features=5000),\n",
        "                   TfidfVectorizer(max_features=5000),\n",
        "                   CountVectorizer(max_features=5000,ngram_range=(1,2))]\n",
        "                                   \n",
        "names=[\"CountVectorizer\",\"TfidfVectorizer\",\"N-gram CountVectorizer\"]\n",
        "\n",
        "\n",
        "for i in range(len(set_of_vectorizers)):\n",
        "  X=set_of_vectorizers[i].fit_transform(X_train)\n",
        "  clf = LogisticRegression(max_iter=90000, C=1).fit(X,y_train)\n",
        "  X_test_v=set_of_vectorizers[i].transform(X_test)\n",
        "  pred_y=clf.predict(X_test_v)\n",
        "  print(f\"{names[i]} accuracy score {accuracy_score(pred_y,y_test)}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CountVectorizer accuracy score 0.8435\n",
            "TfidfVectorizer accuracy score 0.8613333333333333\n",
            "N-gram CountVectorizer accuracy score 0.8375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxY9dUJzE2b6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import string\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import \tWordNetLemmatizer\n",
        "import re\n",
        "\n",
        "def removeStopwords(sentence):\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "  filtered_sentence = [w for w in word_tokenize(sentence)  if not w in stop_words] \n",
        "  return \" \".join(filtered_sentence)\n",
        "\n",
        "def remove_punctuation(sentence):\n",
        "  return sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def remove_html_tags(sentence):\n",
        "  cleanr = re.compile('<.*?>')\n",
        "  cleantext = re.sub(cleanr, '', sentence)\n",
        "  return cleantext\n",
        "\n",
        "# Normalization of words\n",
        "\n",
        "def stemmize(sentence):\n",
        "  stemmer = SnowballStemmer(\"english\")\n",
        "  stemmize_tokens=[stemmer.stem(token) for token in word_tokenize(sentence)]\n",
        "  return \" \".join(stemmize_tokens)\n",
        "\n",
        "def lemmatize(sentence):\n",
        "  wordnet = WordNetLemmatizer()\n",
        "  lem_words=[wordnet.lemmatize(w) for w in word_tokenize(sentence)]\n",
        "  return \" \".join(lem_words)\n",
        "\n",
        "\n",
        "def baseCleaner(sentence):\n",
        "  func_cleaners=[removeStopwords,remove_html_tags,remove_punctuation]\n",
        "  sentence=sentence\n",
        "  for clean_func in func_cleaners:\n",
        "    sentence=clean_func(sentence)\n",
        "  return sentence\n",
        "# normalize and use base cleaners to clean text\n",
        "def stemBC(sentence):\n",
        "  return(stemmize(baseCleaner(sentence)))\n",
        "def lemBC(sentence):\n",
        "  return(lemmatize(baseCleaner(sentence)))\n",
        "\n",
        "# use both normalization techiniuqes considering the order  \n",
        "def both1(sentence):\n",
        "  return(stemmize(lemBC(sentence)))\n",
        "def both2(sentence):\n",
        "  return(lemmatize(stemBC(sentence)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo8W-mqqO5XT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create datasets with clean data\n",
        "stemmize_DS=trainDS.review.apply(stemBC)\n",
        "lemmized_DS=trainDS.review.apply(lemBC)\n",
        "both1_DS=trainDS.review.apply(both1)\n",
        "both2_DS=trainDS.review.apply(both2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eqcHBDpQaXg",
        "colab_type": "code",
        "outputId": "b9d6d0f2-3748-4000-d90e-4ed1c8581a56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "kf = KFold(n_splits=10)\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "experimentDS=[trainDS.review,stemmize_DS,lemmized_DS,both1_DS,both2_DS]\n",
        "\n",
        "for ds in experimentDS:\n",
        "  vectors_train = vectorizer.fit_transform(ds)\n",
        "  logReg=LogisticRegression(max_iter=90000000)\n",
        "  scores = cross_val_score(logReg, vectors_train, trainDS.sentiment, cv=kf)\n",
        "  avg_score = np.mean(scores)\n",
        "  print(avg_score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8668666666666667\n",
            "0.8678000000000001\n",
            "0.8648666666666667\n",
            "0.8678666666666667\n",
            "0.8676999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX3ysjCPNFYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "# forloop iteration testing\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "max = -1\n",
        "index = -1\n",
        "for i in range(0, 0):\n",
        "  vectorizer = TfidfVectorizer(max_features=50000)\n",
        "  vectors_train = vectorizer.fit_transform(X_train)\n",
        "  clf = LogisticRegression(max_iter=90000000).fit(vectors_train, y_train)\n",
        "  vectors_test=vectorizer.transform(X_test)\n",
        "  predicted_values=clf.predict(vectors_test)\n",
        "  print(\"***INDEX***\", i)\n",
        "  print(classification_report(predicted_values,y_test))\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIoH981OqwGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from sklearn.metrics import classification_report\n",
        "vectors_test=vectorizer.transform(X_test)\n",
        "predicted_values=clf.predict(vectors_test)\n",
        "print(classification_report(predicted_values,y_test))\n",
        "\n",
        "vectorRealTest=vectorizer.transform(testDS.review)\n",
        "values=clf.predict(vectorRealTest)\n",
        "df = pd.DataFrame()\n",
        "df[\"id\"]=testDS.id\n",
        "df[\"answer\"]=values\n",
        "\n",
        "print(df)\n",
        "\n",
        "from google.colab import files\n",
        "df.to_csv('filename.csv') \n",
        "#files.download('filename.csv')\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHufyxla3v9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  \"\"\"\n",
        "  # Title: Bernoulli Naive Bayes Classifier\n",
        "  # Author:  Robert M. Johnson \n",
        "  # Date: <date>\n",
        "  # Code version: 1\n",
        "  # Availability: https://mattshomepage.com/articles/2016/Jun/07/bernoulli_nb/\n",
        "  \"\"\"\n",
        "def get_features(text):\n",
        "    return set([w.lower() for w in text.split(\" \")])\n",
        "\n",
        "import numpy as np\n",
        "from math import log\n",
        "\n",
        "from collections import Counter\n",
        "class NaiveBayesB():\n",
        "    def __init__(self):\n",
        "      self._log_priors = None\n",
        "      self._cond_probs = None\n",
        "      self.features = None\n",
        "\n",
        "    def fit(self, documents, labels):\n",
        "\n",
        "        label_counts = Counter(labels)\n",
        "        N = float(sum(label_counts.values()))\n",
        "        self._log_priors = {k: log(v/N) for k, v in label_counts.items()}\n",
        "\n",
        "        X = [set(get_features(d)) for d in documents]\n",
        "        self.features = set([f for features in X for f in features])\n",
        "        self._cond_probs = {l: {f: 0. for f in self.features} for l in self._log_priors}\n",
        "\n",
        "        for x, l in zip(X, labels):\n",
        "            for f in x:\n",
        "                self._cond_probs[l][f] += 1.\n",
        "\n",
        "        for l in self._cond_probs:\n",
        "            N = label_counts[l]\n",
        "            self._cond_probs[l] = {f: (v + 1.) / (N + 2.) for f, v in self._cond_probs[l].items()}\n",
        "\n",
        "    def predict(self, text):\n",
        "      x = get_features(text)\n",
        "      pred_class = None\n",
        "      max_ = float(\"-inf\")\n",
        "\n",
        "      for l in self._log_priors:\n",
        "          log_sum = self._log_priors[l]\n",
        "          for f in self.features:\n",
        "              prob = self._cond_probs[l][f]\n",
        "              log_sum += log(prob if f in x else 1. - prob)\n",
        "          if log_sum > max_:\n",
        "              max_ = log_sum\n",
        "              pred_class = l\n",
        "\n",
        "      return pred_class\n",
        "\n",
        "    def predictDS(self,dataset):\n",
        "      return [self.predict(instance) for instance in dataset]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2RhFde3Jdkrb",
        "outputId": "8abd9e70-7995-4413-8d78-77968a08292a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Loggistic Regression section\n",
        "NUM_FOLDS=10\n",
        "kf = KFold(n_splits=NUM_FOLDS)\n",
        "X=both1_DS\n",
        "y=trainDS.sentiment\n",
        "vectorizer = TfidfVectorizer(max_features=5000).fit(X)\n",
        "logRegression= LogisticRegression(max_iter=90000, C=1)\n",
        "scores = cross_val_score(logRegression, vectorizer.fit_transform(X), y, cv=kf)\n",
        "avg_score = np.mean(scores)\n",
        "print(f\"Logistic Regression result: {avg_score}\")\n",
        "\n",
        "# naive Bayes\n",
        "scores=list()\n",
        "for train_index, test_index in kf.split(X,y):\n",
        "  clf=NaiveBayesB()\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "  # Naive Bayes Model\n",
        "  clf.fit(X_train,y_train)\n",
        "\n",
        "  predictions=clf.predictDS(X_test)\n",
        "\n",
        "  scores.append(accuracy_score(y_test,predictions))\n",
        "\n",
        "result=np.mean(scores)\n",
        "print(f\"Naive Bayes Result:{result}\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8635666666666667\n",
            "0.8464333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G2eB8cSDO0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "ds=[trainDS.review]\n",
        "\n",
        "for x in ds:\n",
        "  cls=LogisticRegression(max_iter=90000)\n",
        "  vectorizer=TfidfVectorizer(max_features=50000)\n",
        "  X=vectorizer.fit_transform(x)\n",
        "  cls.fit(X,trainDS.sentiment)\n",
        "  vectorRealTest=vectorizer.transform(testDS.review)\n",
        "  values=cls.predict(vectorRealTest)\n",
        "  df = pd.DataFrame()\n",
        "  df[\"sentiment\"]=values\n",
        "\n",
        "  from google.colab import files\n",
        "  df.to_csv('filename.csv') \n",
        "  files.download('filename.csv')\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}